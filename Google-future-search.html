<!doctype html>
<html>
<head>
	<meta charset="UTF-8">
	<title>Google the future of search</title>
	<link rel="stylesheet" type="text/css" href="/assets/css/akstyles.css" />
</head>
<body>
<header>
<h1>Art Kavanagh</h1>
<p><a href="https://www.artkavanagh.ie/">Criticism, fiction and other writing</a></p>
<hr>
</header>
<h1>Why Google might just conceivably be the future of search</h1>

<p>Charles Arthur’s <a href="https://theoverspill.wordpress.com/amazon-device-fraud-2fa-start-up-1180#007bba3fe0d1345cfda95a23e448bb0e">Overspill blog recently linked</a> to <a href="https://millionshort.com/about">Million Short</a>, a site that aims to help you find the search results that get lost or buried when you use Google, DuckDuckGo or Bing. It tries to do this by allowing you to discard the first 100, 1,000, 10,000, 100,000 or million sites from the results, or to exclude certain types of site, such as ecommerce. I’ve <a href="https://medium.com/@artkavanagh/eluding-the-algorithm-5fd3d457923c">written before</a> about how algorithmic search has developed to the point where it tends to produce lots of very similar hits, giving the impression that it may be missing many high quality and relevant but idiosyncratic results.</p>

<p>My initial impression of Million Short’s approach is that it’s too crude a measure to turn up the good stuff reliably (or indeed often). When Google first appeared, its algorithm ranked pages on the basis of the number of links from other pages with a high reputation or standing. These factors were proxies for <em>relevance</em>. The more “high quality” links it had, the more likely it was that a page would be worth your while. So was born the industry of Search Engine Optimization.</p>

<p>The existence of sites like Million Short indicates that such proxies for relevance aren’t always very good at finding what we’re looking for. We need something better. My own preference would be for a search engine that rewarded the quality (and, in some cases, the originality) of the <em>writing</em>. The crudeness of the Million Short approach made me think it might be the kind of problem that’s susceptible to a solution based on neural networks, machine learning and big data (AI for short).</p>

<p>It’s widely recognized that <a href="https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/">the quality of Google Translate improved immeasurably</a> when Google introduced an AI model (Neural Machine Translation), in place of its previous statistical approach to translation. Search seems ripe for an analogous change of approach. Instead of proxies for relevance, search could be based on the identification by suitably trained neural networks of writing that is coherent, well informed, thoughful and illuminating. It can’t happen soon enough for me.</p>

<p>With its enormous resources and its experience of building an AI-based machine translation system, as well as its history of dominating the search ecosystem, Google would appear to be better placed to undertake this task than almost any other company. Strange as it might seem, the incumbent may be our best hope for overdue innovative disruption in search.</p>

<p>Posted by <a href="https://www.artkavanagh.ie">Art on 13-Nov-2019</a>.</p>

</body>
</html>